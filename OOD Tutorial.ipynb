{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import model \n",
    "import helper\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-distribution Generalization with ERM\n",
    "The most straightforward way to handle OOD generalization is empirical risk minimization.\n",
    "In short, you can just merge the data from multiple sources (a.k.a. domains, environments and subpopulations) and train a model with them.\n",
    "Previous researches like [DomainBed](https://github.com/facebookresearch/DomainBed) have found that such simple strategy can beat several sophisticatedly designed methods in practical settings.\n",
    "Thus we introduce ERM as our very first baseline to solve the OOD generalization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first set the experimental environment, including random seed, gpu_id and several other arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.fix_seed(0)\n",
    "args = helper.Args()\n",
    "device = torch.device(\"cuda:%d\"%args.gpu_id if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the ERM model (we instantiate it with Resnet18 backbone with substituted classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = model.ERM(args.num_classes, args)\n",
    "my_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the training dataloader with sample NICO++ data (just ignore the domain labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = helper.get_ERM_dataloader(args, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the Resnet18 using backpropagation and print the training loss every 20 iterations.\n",
    "We also test the model on seperate test set and report the test accuracy every 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for step in range(args.num_steps):   \n",
    "    x, y = next(train_dataloader)\n",
    "    mini_batches = [x.to(device), y.to(device)]\n",
    "    step_vals = my_model.update(mini_batches)\n",
    "    if (step+1) % 20 == 0:\n",
    "        log_str = \"Step %d \" % (step+1)\n",
    "        for k, v in step_vals.items():\n",
    "            log_str = log_str + \"%s: %.4f, \" % (k, v)\n",
    "        print(log_str)\n",
    "\n",
    "    if (step+1) % 100 == 0:        \n",
    "        test_dataloader = helper.get_ERM_dataloader(args, 'test')        \n",
    "        accuracy = helper.test(my_model, test_dataloader, device)\n",
    "        print(\"ite: %d, test accuracy: %.4f\" % (step+1, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss generally decreases over time, and the test accuracy is better than random guess (20%), seems good :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Generalization with Mixup\n",
    "\n",
    "Besides the naive training strategy with pooled data, we show another strand of method called Mixup which interpolates minibatches from different domains\n",
    "\n",
    "https://arxiv.org/pdf/2001.00677.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1912.01805.pdf\n",
    "\n",
    "In this method, through the lens of the simple yet effective mixup training, the authors try to implement the mixup across different domain images and labels to achieve the domain robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first intialize the Mixup model. Please refer the the model.py file for the more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = model.Mixup(args.num_classes, args)\n",
    "my_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the training dataloader. Note that different from the ERM dataloader, here we also sample the domain label to perform the mixup across different domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = helper.get_DG_dataloader(args, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training process. We can find that the training loss is higher than that of the ERM training. It is reasonable due to the cross-domain mixup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(args.num_steps):\n",
    "    mini_batches = next(train_dataloader)\n",
    "    step_vals = my_model.update(mini_batches)\n",
    "    if (step+1) % 20 == 0:\n",
    "        log_str = \"Step %d \" % (step+1)\n",
    "        for k, v in step_vals.items():\n",
    "            log_str = log_str + \"%s: %.4f, \" % (k, v)\n",
    "        print(log_str)\n",
    "\n",
    "    if (step+1) % 100 == 0:\n",
    "        test_dataloader = helper.get_ERM_dataloader(args, 'test')\n",
    "        accuracy = helper.test(my_model, test_dataloader, device)\n",
    "        print(\"ite: %d, test accuracy: %.4f\" % (step+1, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final validation accuracy looks like ok :)\n",
    "\n",
    "In practice, you can either use the domain labels as extra information or just ignore them, depends on your actual applications.\n",
    "We show both two strands of methods here to let you know there are generally two paradigms on handling OOD problem.\n",
    "Feel free to play with NICO++ and happy researching!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
